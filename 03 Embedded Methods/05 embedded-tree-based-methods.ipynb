{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c22226",
   "metadata": {},
   "source": [
    "### Feature Selection Using Tree-Based Algorithms\n",
    "\n",
    "Tree-based models, such as **Decision Trees**, **Random Forests**, and **Gradient Boosted Trees**, can be used for **feature selection** by leveraging their ability to evaluate the **importance of each feature** while building the model.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1How it works\n",
    "\n",
    "1. Fit a **tree-based model** (e.g., Random Forest) on your dataset.\n",
    "2. Each split in a tree selects the feature that **best reduces impurity** (e.g., Gini, entropy, MSE).\n",
    "3. Features that contribute more to reducing impurity across all trees are assigned a **higher importance score**.\n",
    "4. Features can be **ranked by importance**, and less important features can be removed.\n",
    "\n",
    "> In Random Forests, feature importance is usually computed as the **mean decrease in impurity** or via **permutation importance**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Pros\n",
    "\n",
    "- **Handles non-linear relationships** between features and target.\n",
    "- **Works with both classification and regression**.\n",
    "- **No need for feature scaling**.\n",
    "- Can automatically handle **categorical and numerical features**.\n",
    "- Captures **feature interactions** naturally.\n",
    "- Provides a **ranking of features**, making selection straightforward.\n",
    "\n",
    "---\n",
    "\n",
    "#### Cons\n",
    "\n",
    "- Can be **biased toward features with more categories or higher cardinality**.\n",
    "- Less effective if **many features are highly correlated**, as importance may be split among them.\n",
    "- **Permutation-based importance** is more reliable but computationally expensive.\n",
    "- Does not inherently provide **sparse selection** like L1-based models (features are ranked, not zeroed out).\n",
    "- Feature importance may **change between runs** if the model is not deterministic or has low number of trees.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical workflow\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Fit Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Rank features\n",
    "feature_ranking = sorted(zip(X_train.columns, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select top k features\n",
    "top_features = [f for f, imp in feature_ranking[:10]]\n",
    "\n",
    "```\n",
    "\n",
    "> It is easy to use `SelectFromModel`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d62af7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
