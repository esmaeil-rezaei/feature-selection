{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c15335c3",
   "metadata": {},
   "source": [
    "# Hilbert–Schmidt Independence Criterion (HSIC): A Clear Numerical Example\n",
    "\n",
    "This section provides a **step-by-step numerical explanation** of **HSIC** using a nonlinear example. HSIC is a **kernel-based, nonparametric dependence measure** that is widely used for **feature selection**, especially in **time series and high-dimensional settings**.\n",
    "\n",
    "Unlike Mutual Information (MI), HSIC:\n",
    "\n",
    "- Does **not** estimate probability densities\n",
    "- Does **not** require discretization or kNN\n",
    "- Measures dependence via **kernels and geometry in RKHS**\n",
    "\n",
    "## Example Setup\n",
    "\n",
    "We observe one feature $X$ and one target $Y$ over four time points:\n",
    "\n",
    "| i   | $X_i$ | $Y_i$ |\n",
    "| --- | ----- | ----- |\n",
    "| 1   | 1     | 1     |\n",
    "| 2   | 2     | 4     |\n",
    "| 3   | 3     | 9     |\n",
    "| 4   | 4     | 16    |\n",
    "\n",
    "This is again a **nonlinear relationship**:  \n",
    "$Y = X^2$\n",
    "\n",
    "## Step 1 — Choose Kernels\n",
    "\n",
    "HSIC requires a kernel for $X$ and a kernel for $Y$.\n",
    "\n",
    "For simplicity, we use the **linear kernel**:\n",
    "\n",
    "$$\n",
    "k(x_i, x_j) = x_i x_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "l(y_i, y_j) = y_i y_j\n",
    "$$\n",
    "\n",
    "> Note: In practice, **Gaussian (RBF) kernels** are more common, but linear kernels make the computations transparent.\n",
    "\n",
    "## Step 2 — Compute Kernel Matrices\n",
    "\n",
    "### Kernel Matrix for $X$\n",
    "\n",
    "$$\n",
    "K_{ij} = X_i X_j\n",
    "$$\n",
    "\n",
    "| \\(K\\) | 1   | 2   | 3   | 4   |\n",
    "| ----- | --- | --- | --- | --- |\n",
    "| 1     | 1   | 2   | 3   | 4   |\n",
    "| 2     | 2   | 4   | 6   | 8   |\n",
    "| 3     | 3   | 6   | 9   | 12  |\n",
    "| 4     | 4   | 8   | 12  | 16  |\n",
    "\n",
    "### Kernel Matrix for $Y$\n",
    "\n",
    "$$\n",
    "L_{ij} = Y_i Y_j\n",
    "$$\n",
    "\n",
    "| \\(L\\) | 1   | 2   | 3   | 4   |\n",
    "| ----- | --- | --- | --- | --- |\n",
    "| 1     | 1   | 4   | 9   | 16  |\n",
    "| 2     | 4   | 16  | 36  | 64  |\n",
    "| 3     | 9   | 36  | 81  | 144 |\n",
    "| 4     | 16  | 64  | 144 | 256 |\n",
    "\n",
    "## Step 3 — Center the Kernel Matrices\n",
    "\n",
    "HSIC operates on **centered kernel matrices**.\n",
    "\n",
    "Define the centering matrix:\n",
    "\n",
    "$$\n",
    "H = I - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top\n",
    "$$\n",
    "\n",
    "For \\(n = 4\\):\n",
    "\n",
    "$$\n",
    "H =\n",
    "\\begin{bmatrix}\n",
    "0.75 & -0.25 & -0.25 & -0.25 \\\\\n",
    "-0.25 & 0.75 & -0.25 & -0.25 \\\\\n",
    "-0.25 & -0.25 & 0.75 & -0.25 \\\\\n",
    "-0.25 & -0.25 & -0.25 & 0.75\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Centered kernels:\n",
    "\n",
    "$$\n",
    "\\tilde{K} = HKH\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{L} = HLH\n",
    "$$\n",
    "\n",
    "### Centered Kernel Matrix for $X$\n",
    "\n",
    "| $\\tilde{K}$ | 1    | 2    | 3    | 4    |\n",
    "| ----------- | ---- | ---- | ---- | ---- |\n",
    "| 1           | 3.5  | 1.5  | -0.5 | -4.5 |\n",
    "| 2           | 1.5  | 0.5  | -0.5 | -1.5 |\n",
    "| 3           | -0.5 | -0.5 | 0.5  | 0.5  |\n",
    "| 4           | -4.5 | -1.5 | 0.5  | 5.5  |\n",
    "\n",
    "### Centered Kernel Matrix for $Y$\n",
    "\n",
    "| $\\tilde{L}$ | 1     | 2     | 3     | 4     |\n",
    "| ----------- | ----- | ----- | ----- | ----- |\n",
    "| 1           | 73.5  | 25.5  | -18.5 | -80.5 |\n",
    "| 2           | 25.5  | 7.5   | -6.5  | -26.5 |\n",
    "| 3           | -18.5 | -6.5  | 7.5   | 17.5  |\n",
    "| 4           | -80.5 | -26.5 | 17.5  | 89.5  |\n",
    "\n",
    "## Step 4 — Compute HSIC\n",
    "\n",
    "The empirical HSIC is defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{HSIC}(X,Y)\n",
    "=\n",
    "\\frac{1}{(n-1)^2}\n",
    "\\mathrm{trace}(\\tilde{K}\\tilde{L})\n",
    "$$\n",
    "\n",
    "### Matrix Product and Trace\n",
    "\n",
    "Compute:\n",
    "\n",
    "$$\n",
    "\\mathrm{trace}(\\tilde{K}\\tilde{L})\n",
    "=\n",
    "\\sum_{i,j} \\tilde{K}_{ij}\\tilde{L}_{ij}\n",
    "$$\n",
    "\n",
    "For this example:\n",
    "\n",
    "$$\n",
    "\\sum_{i,j} \\tilde{K}_{ij}\\tilde{L}_{ij} = 2260\n",
    "$$\n",
    "\n",
    "With \\(n = 4\\):\n",
    "\n",
    "$$\n",
    "\\mathrm{HSIC}(X,Y) = \\frac{2260}{9} \\approx 251.1\n",
    "$$\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "- HSIC $> 0$ indicates dependence\n",
    "- HSIC $= 0$ **if and only if** $X$ and $Y$ are independent (with characteristic kernels)\n",
    "- The large value reflects **strong nonlinear dependence**\n",
    "\n",
    "## Why HSIC is Powerful for Time Series Feature Selection\n",
    "\n",
    "- Fully **nonparametric**\n",
    "- Captures **linear and nonlinear** dependencies\n",
    "- Avoids density estimation (unlike MI)\n",
    "- Works naturally with **lagged variables**\n",
    "- Stable in **high dimensions**\n",
    "\n",
    "## Comparison to MI and dCor\n",
    "\n",
    "| Method               | Density Estimation | Captures Nonlinearity | Popular in TS FS |\n",
    "| -------------------- | ------------------ | --------------------- | ---------------- |\n",
    "| Mutual Information   | Yes (kNN / KDE)    | Yes                   | Very high        |\n",
    "| Distance Correlation | No                 | Yes                   | High             |\n",
    "| HSIC                 | No (kernel-based)  | Yes                   | Very high        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe84d4",
   "metadata": {},
   "source": [
    "`scikit‑learn` does not have a built‑in HSIC function, but you can implement HSIC in Python using kernel matrices (e.g., RBF/Gaussian kernels) and basic linear algebra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8796c8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_kernels, pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f378df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsic(X, Y, kernel=\"rbf\", gamma=None):\n",
    "    \"\"\"\n",
    "    Compute the Hilbert-Schmidt Independence Criterion (HSIC)\n",
    "    between X and Y using kernel matrices.\n",
    "\n",
    "    Args:\n",
    "        X: array of shape (n_samples, n_features)\n",
    "        Y: array of shape (n_samples, m_features)\n",
    "        kernel: kernel type passed to sklearn.metrics.pairwise_kernels\n",
    "        gamma: kernel parameter for RBF (if None, 1/n_features is used)\n",
    "\n",
    "    Returns:\n",
    "        HSIC value (float)\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "\n",
    "    # Compute kernel (Gram) matrices\n",
    "    Kx = pairwise_kernels(X, metric=kernel, gamma=gamma)\n",
    "    Ky = pairwise_kernels(Y, metric=kernel, gamma=gamma)\n",
    "\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "\n",
    "    Kx_c = H @ Kx @ H\n",
    "    Ky_c = H @ Ky @ H\n",
    "\n",
    "    hsic_val = np.trace(Kx_c @ Ky_c) / ((n - 1) ** 2)\n",
    "    return hsic_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb120873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSIC value between X and Y: 0.01117\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "n = 200\n",
    "\n",
    "X = np.cumsum(np.random.randn(n))\n",
    "X = X.reshape(-1, 1)\n",
    "Y = np.sin(0.1 * X) + 0.1 * np.random.randn(n, 1)\n",
    "\n",
    "hsic_value = hsic(X, Y, kernel=\"rbf\", gamma=1.0 / X.shape[1])\n",
    "print(f\"HSIC value between X and Y: {hsic_value:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68553b50",
   "metadata": {},
   "source": [
    "The magnitude of HSIC/NHSIC depends heavily on the kernel choice and bandwidth (gamma). If your data has large variance or oscillations, the RBF kernel may still produce small values because the pairwise distances in the kernel matrix are large, so $exp(-\\gamma\\times dist^2)$ is very small.\n",
    "\n",
    "We need to:\n",
    "\n",
    "- Normalize values\n",
    "- Find gamma using pairwise Euclidean distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91ade3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = StandardScaler()\n",
    "scaler_Y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "Y_scaled = scaler_Y.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1787b37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4867670629291414"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute pairwise Euclidean distances\n",
    "dists = pairwise_distances(X_scaled)\n",
    "median_dist = np.median(dists)\n",
    "gamma = 1 / (2 * median_dist ** 2)\n",
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33c553e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSIC value between X and Y: 0.00896\n"
     ]
    }
   ],
   "source": [
    "hsic_value = hsic(X, Y, kernel=\"rbf\", gamma=gamma)\n",
    "print(f\"HSIC value between X and Y: {hsic_value:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f6af19",
   "metadata": {},
   "source": [
    "### HSIC for lag selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcd25d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag 1: HSIC = 0.00844\n",
      "Lag 2: HSIC = 0.00812\n",
      "Lag 3: HSIC = 0.00778\n",
      "Lag 4: HSIC = 0.00748\n",
      "Lag 5: HSIC = 0.00730\n"
     ]
    }
   ],
   "source": [
    "lags = 5\n",
    "hsic_scores = []\n",
    "for lag in range(1, lags + 1):\n",
    "    X_lag = np.roll(X, lag, axis=0)\n",
    "    val = hsic(X_lag, Y, kernel=\"rbf\", gamma=gamma)\n",
    "    hsic_scores.append(val)\n",
    "    print(f\"Lag {lag}: HSIC = {val:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc4725",
   "metadata": {},
   "source": [
    "- Higher HSIC ⇒ stronger dependency between X and Y.\n",
    "\n",
    "- Works well for nonlinear time series relationships where Pearson correlation fails.\n",
    "\n",
    "> Key point: HSIC values are often much smaller than `MI` or `dCor`, even when variables are clearly dependent. This is normal. So you have to interpret them differently from `MI` or `dCor`. We need to rank lags and select the informative ones.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
