{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f4a902a",
   "metadata": {},
   "source": [
    "# Feature Selection Methods for Time Series Data\n",
    "\n",
    "Feature selection (FS) has been widely studied in the machine learning literature, particularly for **univariate time series** (single variable over time) **multivariate non-time-series data** (many features observed independently of time). Classical feature selection techniques—filter, wrapper, and embedded methods—have demonstrated strong performance in these domains. Examples include:\n",
    "\n",
    "- **Filter methods:** Mutual Information, Correlation, ANOVA\n",
    "- **Wrapper methods:** Recursive Feature Elimination (RFE)\n",
    "- **Embedded methods:** Lasso, tree-based importance\n",
    "\n",
    "However, relatively **limited attention has been given to feature selection for multivariate time series (MTS)** data, where features not only have relevance at each time point but also exhibit _temporal dynamics_ across time. The key challenge for time series is that temporal structure must be preserved when features are evaluated:\n",
    "\n",
    "- Directly applying classical methods (e.g., RFE) typically requires **vectorizing the time series**, which destroys temporal relationships\n",
    "- Many state-of-the-art selection algorithms assume i.i.d. data, violating the dependence structure inherent in time series\n",
    "\n",
    "### Mutual Information at the Heart of Time Series FS\n",
    "\n",
    "Across the literature, **Mutual Information (MI)** has emerged as a **central building block** for feature selection in time series:\n",
    "\n",
    "- MI quantifies **nonlinear dependency** between variables\n",
    "- For time series, MI is used to measure **how much knowing a feature (or its lags) reduces uncertainty in the target**\n",
    "- A high MI indicates that a feature carries useful information about the target behavior over time\n",
    "\n",
    "MI has been favored because:\n",
    "\n",
    "- It captures **both linear and non-linear relationships**\n",
    "- It does not rely on Gaussian assumptions\n",
    "- It can be adapted to multivariate settings with lagged representations\n",
    "\n",
    "### Evolution of MI Estimation in the Literature\n",
    "\n",
    "**Early Approaches: Discretization**\n",
    "\n",
    "In earlier research (pre-2010), MI was often estimated by:\n",
    "\n",
    "- **Discretizing continuous variables**\n",
    "- Building **contingency tables**\n",
    "- Using histograms or fixed bins to approximate densities\n",
    "\n",
    "Pros:\n",
    "\n",
    "- Conceptually simple\n",
    "- Easy to compute by hand or with basic tools\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Loss of precision\n",
    "- Sensitive to bin size\n",
    "- Inefficient for high-dimensional continuous data\n",
    "\n",
    "As a result, discretization is **no longer the preferred method** for MI estimation in modern time series feature selection.\n",
    "\n",
    "**Modern MI: kNN Estimators**\n",
    "\n",
    "Today’s research mainly uses **k-Nearest Neighbors (kNN) based MI estimators**, such as:\n",
    "\n",
    "- Kraskov estimator\n",
    "- kNN entropy estimators adapted for MI\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Works directly on **continuous variables**\n",
    "- No discretization needed\n",
    "- Captures **local density structure**\n",
    "- Performs well with small to moderate sample sizes\n",
    "- In discretization, distances define local densities\n",
    "\n",
    "This approach is widely used in recent time series FS papers because it maintains continuous information and avoids arbitrary binning.\n",
    "\n",
    "### Lagged Variables and Multivariate Time Series\n",
    "\n",
    "A common strategy in time series feature selection is to **include lagged versions of variables**:\n",
    "\n",
    "- Instead of treating each feature as a static column, researchers create:\n",
    "  - $X(t)$, $X(t-1)$, $X(t-2)$, … up to some maximum lag\n",
    "- Lagged features allow models to capture **temporal dependencies**\n",
    "- Feature selection then evaluates not only **which features** are important, but **which lags** are important\n",
    "\n",
    "Given a feature $X$, with maximum lag $L$, the feature set becomes:\n",
    "\n",
    "$$\n",
    "\\{ X(t), X(t-1), X(t-2), \\ldots, X(t-L) \\}\n",
    "$$\n",
    "\n",
    "In this setting, three different elimination strategies are commonly discussed:\n",
    "\n",
    "1. **Eliminate only the eliminated tag (lag)**\n",
    "\n",
    "   - Remove a specific lag that is uninformative\n",
    "   - Keep other lags of the same feature\n",
    "   - > _The most popular and widely used_\n",
    "\n",
    "2. **Eliminate up to the candidate lag**\n",
    "\n",
    "   - Remove all lags up to a particular lag level\n",
    "   - E.g., remove $X(t-1)$ and all earlier lags\n",
    "\n",
    "3. **Eliminate all lags of a feature**\n",
    "   - If no lag of a feature is informative,\n",
    "   - Remove the entire feature from the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70292b",
   "metadata": {},
   "source": [
    "# Example: Mutual Information via Discretization\n",
    "\n",
    "### Problem setup\n",
    "\n",
    "Suppose we study whether blood pressure (BP) is informative about a binary health outcome.\n",
    "\n",
    "- Feature X: Blood Pressure (continuous)\n",
    "- Target Y: Disease status (binary)\n",
    "\n",
    "We discretize BP into categories.\n",
    "\n",
    "### Step 1: Raw data\n",
    "\n",
    "| Sample | BP (mmHg) | Disease Y |\n",
    "| ------ | --------- | --------- |\n",
    "| 1      | 110       | 0         |\n",
    "| 2      | 115       | 0         |\n",
    "| 3      | 120       | 0         |\n",
    "| 4      | 130       | 1         |\n",
    "| 5      | 135       | 1         |\n",
    "| 6      | 140       | 1         |\n",
    "| 7      | 145       | 1         |\n",
    "| 8      | 150       | 1         |\n",
    "\n",
    "Total samples: N = 8\n",
    "\n",
    "### Step 2: Discretize the feature\n",
    "\n",
    "We discretize BP into two bins:\n",
    "\n",
    "- Low BP (L): $BP < 130$\n",
    "- High BP (H): $BP >= 130$\n",
    "\n",
    "### Step 3: Construct the contingency table\n",
    "\n",
    "| X (BP category) | Y=0 | Y=1 | Total |\n",
    "| --------------- | --- | --- | ----- |\n",
    "| L               | 3   | 0   | 3     |\n",
    "| H               | 0   | 5   | 5     |\n",
    "| Total           | 3   | 5   | 8     |\n",
    "\n",
    "### Step 4: Convert counts to probabilities\n",
    "\n",
    "Marginal probabilities:\n",
    "\n",
    "$$\n",
    "P(X=L) = 3/8 ,  P(X=H) = 5/8\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(Y=0) = 3/8 ,  P(Y=1) = 5/8\n",
    "$$\n",
    "\n",
    "Joint probabilities:\n",
    "\n",
    "$$\n",
    "P(L,0) = 3/8 , P(L,1) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(H,0) = 0 , P(H,1) = 5/8\n",
    "$$\n",
    "\n",
    "### Step 5: Mutual Information definition\n",
    "\n",
    "$$\n",
    "I(X;Y) = \\sum_x \\sum_y P(x,y) \\text{log}_2 \\left(\\frac{P(x,y)}{P(x) P(y)}\\right)\n",
    "$$\n",
    "\n",
    "After omputing each **non-zero term:**\n",
    "\n",
    "$$\n",
    "I(X;Y) = 0.531 + 0.424 = 0.955\n",
    "$$\n",
    "\n",
    "> I(X;Y) = 0.955 indicates strong dependency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20999bc9",
   "metadata": {},
   "source": [
    "# Mutual Information via kNN (k = 2) — Using kNN\n",
    "\n",
    "We use the SAME dataset as before but replace KNN with discretization.\n",
    "\n",
    "### Step 1: Data\n",
    "\n",
    "We have one continuous feature X (Blood Pressure)\n",
    "and one discrete target Y (Disease).\n",
    "\n",
    "| i   | X (BP) | Y   |\n",
    "| --- | ------ | --- |\n",
    "| 1   | 110    | 0   |\n",
    "| 2   | 115    | 0   |\n",
    "| 3   | 120    | 0   |\n",
    "| 4   | 130    | 1   |\n",
    "| 5   | 135    | 1   |\n",
    "| 6   | 140    | 1   |\n",
    "| 7   | 145    | 1   |\n",
    "| 8   | 150    | 1   |\n",
    "\n",
    "Total samples:\n",
    "N = 8\n",
    "\n",
    "k = 2\n",
    "\n",
    "### Step 2: MI estimator (KSG-style, mixed continuous–discrete)\n",
    "\n",
    "We use the commonly cited kNN MI estimator:\n",
    "\n",
    "$$\n",
    "I(X;Y) = ψ(k) + ψ(N) - ⟨ ψ(n_x + 1) + ψ(n_y + 1) ⟩\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ψ(.)$ is the digamma function\n",
    "- $n_x$ = number of neighbors within ε in X-space\n",
    "- $n_y$ = number of neighbors with same Y-label within ε\n",
    "- $\\epsilon$ = distance to k-th nearest neighbor in joint space\n",
    "\n",
    "### Step 3: Distance in joint space\n",
    "\n",
    "Joint space distance:\n",
    "\n",
    "- $X$ uses absolute distance\n",
    "- $Y$ must be EXACT match (distance = 0 if same class, infinite otherwise)\n",
    "\n",
    "Thus:\n",
    "Only points with same $Y$ are considered neighbors.\n",
    "\n",
    "### Step 4: Compute ε for each point (k = 2)\n",
    "\n",
    "We find the 2nd nearest neighbor WITH SAME $Y$.\n",
    "\n",
    "##### For Y = 0 (points 1–3)\n",
    "\n",
    "X values: 110, 115, 120\n",
    "\n",
    "Distances:\n",
    "\n",
    "- Point 110:\n",
    "  neighbors: 115 (5), 120 (10)\n",
    "  ε = 10\n",
    "\n",
    "- Point 115:\n",
    "  neighbors: 110 (5), 120 (5)\n",
    "  ε = 5\n",
    "\n",
    "- Point 120:\n",
    "  neighbors: 115 (5), 110 (10)\n",
    "  ε = 10\n",
    "\n",
    "##### For Y = 1 (points 4–8)\n",
    "\n",
    "X values: 130, 135, 140, 145, 150\n",
    "\n",
    "- Point 130:\n",
    "  neighbors: 135 (5), 140 (10)\n",
    "  ε = 10\n",
    "\n",
    "- Point 135:\n",
    "  neighbors: 130 (5), 140 (5)\n",
    "  ε = 5\n",
    "\n",
    "- Point 140:\n",
    "  neighbors: 135 (5), 145 (5)\n",
    "  ε = 5\n",
    "\n",
    "- Point 145:\n",
    "  neighbors: 140 (5), 150 (5)\n",
    "  ε = 5\n",
    "\n",
    "- Point 150:\n",
    "  neighbors: 145 (5), 140 (10)\n",
    "  ε = 10\n",
    "\n",
    "### Step 5: Count $n_x$ and $n_y$\n",
    "\n",
    "For each point:\n",
    "\n",
    "- $n_x$ = number of points within $ε$ distance in $X$ (excluding itself)\n",
    "- $n_y$ = number of points with same Y within $ε$ distance\n",
    "\n",
    "Since $Y$ is fixed inside $ε$:\n",
    "$n_x = n_y$ for all points\n",
    "\n",
    "##### Counts for Y = 0\n",
    "\n",
    "| X   | ε   | neighbors within $ | X_i - X_j | <ε$ | $n_x = n_y$ |\n",
    "| --- | --- | ------------------ | --------- | --- | ----------- |\n",
    "| 110 | 10  | 115,120            | 2         |\n",
    "| 115 | 5   | 110,120            | 2         |\n",
    "| 120 | 10  | 115,110            | 2         |\n",
    "\n",
    "##### Counts for Y = 1\n",
    "\n",
    "| X   | ε   | neighbors within $ | X_i - X_j | <ε$ | $n_x = n_y$ |\n",
    "| --- | --- | ------------------ | --------- | --- | ----------- |\n",
    "| 130 | 10  | 135,140            | 2         |\n",
    "| 135 | 5   | 130,140            | 2         |\n",
    "| 140 | 5   | 135,145            | 2         |\n",
    "| 145 | 5   | 140,150            | 2         |\n",
    "| 150 | 10  | 145,140            | 2         |\n",
    "\n",
    "### Step 6: Plug into MI formula\n",
    "\n",
    "We now compute expectations.\n",
    "\n",
    "For ALL points:\n",
    "\n",
    "- $n_x = 2$\n",
    "- $n_y = 2$\n",
    "\n",
    "Thus:\n",
    "\n",
    "- $ψ(n_x + 1) = ψ(3)$\n",
    "- $ψ(n_y + 1) = ψ(3)$\n",
    "\n",
    "### Step 7: Digamma values (approximate)\n",
    "\n",
    "- $ψ(2) ≈ 0.423$\n",
    "- $ψ(3) ≈ 0.923$\n",
    "- $ψ(8) ≈ 2.015$\n",
    "\n",
    "### Step 8: Compute MI\n",
    "\n",
    "$$\n",
    "I(X;Y) = ψ(2) + ψ(8) - [ ψ(3) + ψ(3) ]\n",
    "$$\n",
    "\n",
    "$$\n",
    "I(X;Y) = 0.423 + 2.015 - (0.923 + 0.923)\n",
    "$$\n",
    "\n",
    "$$\n",
    "I(X;Y) = 2.438 - 1.846 = 0.592 \\text{ bits}\n",
    "$$\n",
    "\n",
    "Convert to bits:\n",
    "\n",
    "$$\n",
    "I_{bits} = 0.592 / ln(2) ≈ 0.855 \\text{ bits}\n",
    "$$\n",
    "\n",
    "- $MI ≈ 0.855$ bits → strong dependence\n",
    "- Result is close to discretized MI (≈ 0.955)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
