{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0fbb7d6",
   "metadata": {},
   "source": [
    "### What is feature selection\n",
    "\n",
    "Feature selection is the process of choosing a subset of relevant input variables (features) from the original set, while discarding irrelevant, redundant, or noisy features, without transforming the features themselves.\n",
    "\n",
    "### Feature selection vs feature engineering vs feature extraction\n",
    "\n",
    "##### Feature selection\n",
    "\n",
    "In feature selection, we select columns without changing their values. The goal is to identify features that are not redundant and not noise, and to remove those that do not contribute meaningful information to the task.\n",
    "\n",
    "To do this, we use different techniques, such as LASSO, where features with zero (or near-zero) coefficients are removed. Importantly, the original feature values remain unchanged; we only decide whether a column is kept or discarded.\n",
    "\n",
    "By removing features, dimensionality is reduced, but dimensionality reduction is not the primary goal. The reduction may be small (removing one or two variables) or large (removing many variables), depending on the data. The main objective is relevance and non-redundancy, not compression.\n",
    "\n",
    "##### Feature extraction\n",
    "\n",
    "In feature extraction, we transform the data into a new feature space. Methods such as PCA and SVD are used to reshape the original variables into new features that better explain the structure or variance of the data.\n",
    "\n",
    "These methods are explicit dimensionality reduction algorithms. The resulting features are combinations of the original variables, and therefore feature values change, and the original columns are no longer preserved.\n",
    "\n",
    "Thus, dimensionality reduction methods are a subset of feature extraction methods, where the objective is to represent the data using fewer, transformed variables.\n",
    "\n",
    "##### Feature engineering\n",
    "\n",
    "In feature engineering, we create new variables that did not originally exist in the dataset, typically using domain knowledge.\n",
    "\n",
    "For example, given weight and height, we can create BMI as a new variable. These engineered features may improve model performance or interpretability and can coexist with the original features.\n",
    "\n",
    "##### Dimensionality reduction\n",
    "\n",
    "Dimensionality reduction is the process of reducing the number of variables (dimensions) in a dataset by transforming the data into a lower-dimensional space.\n",
    "\n",
    "The primary goal is to reduce the dimension itself, typically to:\n",
    "\n",
    "- lower computational cost,\n",
    "\n",
    "- mitigate the curse of dimensionality,\n",
    "\n",
    "- remove noise,\n",
    "\n",
    "- enable visualization or faster learning.\n",
    "\n",
    "To achieve this, dimensionality reduction methods change the feature values by constructing new variables that summarize or combine the original ones. As a result, the original features are not preserved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcd3d3e",
   "metadata": {},
   "source": [
    "#### Why feature selection is needed\n",
    "\n",
    "1. Improves generalization\n",
    "\n",
    "   - Irrelevant features increase variance\n",
    "\n",
    "   - Removing them reduces overfitting\n",
    "\n",
    "2. Removes redundancy\n",
    "\n",
    "   - Highly correlated features provide duplicate information\n",
    "\n",
    "   - Selection keeps one representative feature\n",
    "\n",
    "3. Improves interpretability\n",
    "\n",
    "   - Models become easier to explain and trust\n",
    "\n",
    "   - Critical in healthcare, finance, and scientific studies\n",
    "\n",
    "4. Reduces computational cost\n",
    "\n",
    "   - Faster training and inference\n",
    "\n",
    "   - Lower memory usage\n",
    "\n",
    "5. Improves model stability\n",
    "\n",
    "   - Fewer noisy features ‚Üí more stable coefficients and predictions\n",
    "\n",
    "6. Works well with limited data\n",
    "\n",
    "   - When ùëõ ‚â™ ùëë feature selection is often essential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4218f534",
   "metadata": {},
   "source": [
    "#### Feature selection methods\n",
    "\n",
    "Feature selection methods can be divided into three main categories, based on how the selection is performed and how much the learning model is involved.\n",
    "\n",
    "- Filter method\n",
    "- Wrapper methods\n",
    "- Embedding methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add38c7b",
   "metadata": {},
   "source": [
    "#### 1. Filter methods\n",
    "\n",
    "Filter methods select features independently of any machine learning model.\n",
    "They rely on statistical properties of the data to measure the relevance of each feature.\n",
    "\n",
    "- Features are ranked or scored using statistical criteria.\n",
    "\n",
    "- Model training is not involved.\n",
    "\n",
    "- Fast and scalable to high-dimensional data.\n",
    "\n",
    "**Examples**\n",
    "\n",
    "- Correlation coefficients\n",
    "\n",
    "- Mutual information\n",
    "\n",
    "- Chi-square test\n",
    "\n",
    "- Variance threshold\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- Computationally efficient\n",
    "\n",
    "- Model-**agnostic** (_means a method does not depend on any specific learning algorithm and can be applied with any model_)\n",
    "\n",
    "- Good for initial screening\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- Ignore feature interactions\n",
    "\n",
    "- May select redundant features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f2af71",
   "metadata": {},
   "source": [
    "#### 2. Wrapper methods\n",
    "\n",
    "Wrapper methods evaluate feature subsets by training and testing a model repeatedly. Feature selection is treated as a search problem over subsets. **They use a greedy algorithm to search**.\n",
    "\n",
    "- Model performance directly guides feature selection.\n",
    "\n",
    "- Can capture feature interactions.\n",
    "\n",
    "- Computationally expensive.\n",
    "\n",
    "**Examples**\n",
    "\n",
    "- Forward selection (add features one-by-one)\n",
    "\n",
    "- Backward elimination (starts with all features and removes one or more each time)\n",
    "\n",
    "  - simple backward model\n",
    "    - uses p-value metric\n",
    "    - sometimes model-agnostic\n",
    "  - Recursive Feature Elimination (RFE)\n",
    "    - model-based feature importance\n",
    "    - model-dependent (requires a trained model)\n",
    "\n",
    "- Exhaustive feature search (uses all combinations of features)\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- Often higher predictive performance\n",
    "\n",
    "- Accounts for feature interactions\n",
    "\n",
    "- The selected features may not perform well for other ML models\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- High computational cost\n",
    "\n",
    "- Risk of overfitting on small datasets\n",
    "\n",
    "**Stopping Criteria**\n",
    "\n",
    "1. Predefined number of features\n",
    "\n",
    "   - Stop when a specific number of features remains.\n",
    "\n",
    "2. Performance threshold\n",
    "\n",
    "   - Stop when model performance reaches a desired level.\n",
    "\n",
    "   Example: Stop if validation accuracy ‚â• 95%.\n",
    "\n",
    "3. No further improvement\n",
    "\n",
    "   - Stop when removing or adding more features does not improve performance.\n",
    "\n",
    "   - Common in RFE: continue until removing the next feature degrades validation score.\n",
    "\n",
    "4. Maximum iterations\n",
    "\n",
    "   - Stop after a fixed number of steps to prevent excessive computation.\n",
    "\n",
    "5. Minimum improvement delta\n",
    "\n",
    "   - Stop if the change in performance between iterations is below a small threshold (e.g., Œî accuracy < 0.001).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df92bf",
   "metadata": {},
   "source": [
    "#### 3. Embedded methods\n",
    "\n",
    "Embedded methods perform feature selection as part of the model training process itself.\n",
    "\n",
    "- Selection happens during optimization.\n",
    "\n",
    "- Balance efficiency and performance.\n",
    "\n",
    "- Common in modern ML pipelines.\n",
    "\n",
    "**Examples**\n",
    "\n",
    "- LASSO (L1 regularization)\n",
    "\n",
    "- Elastic Net\n",
    "\n",
    "- Tree-based models (feature importance)\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- Computationally efficient\n",
    "\n",
    "- Model-aware selection\n",
    "\n",
    "- Widely used in industry\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- Model-dependent\n",
    "\n",
    "- Selection tied to model assumptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1537b81",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
