{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ce1789",
   "metadata": {},
   "source": [
    "As been discussed, all of the methods explored so far, such as ANOVA F-tests and SelectKBest based on F-scores, are widely used in **genomic and biological research**. In these fields, the goal is often to **rank features** (e.g., genes) according to their statistical association with a phenotype, and the datasets typically have **very high dimensionality and relatively small sample sizes**, which makes univariate filtering methods practical and interpretable.\n",
    "\n",
    "However, these methods are **less commonly used in general machine learning problems** because they:\n",
    "\n",
    "- Consider **each feature independently**, ignoring interactions or correlations between features.\n",
    "- Assume **linear relationships** (in the case of ANOVA F-tests), which may not hold for complex datasets.\n",
    "- May not align with **model-specific objectives**, where predictive performance depends on multivariate relationships.\n",
    "\n",
    "# Wrapper Methods for Feature Selection\n",
    "\n",
    "**Wrapper methods** are a class of feature selection techniques that evaluate subsets of features based on the **performance of a predictive model**. Unlike filter methods, which rely on statistical measures, wrappers **use the model itself as a black box** to decide which features are most relevant.\n",
    "\n",
    "---\n",
    "\n",
    "### How Wrapper Methods Work\n",
    "\n",
    "1. **Select a subset of features**.\n",
    "2. **Train a model** (e.g., linear regression, decision tree) using only that subset.\n",
    "3. **Evaluate model performance** using a metric such as accuracy, RÂ², RMSE, or cross-validated score.\n",
    "4. **Use the performance to guide selection**:\n",
    "   - Keep features that improve performance.\n",
    "   - Remove or discard features that do not.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- Takes into account **feature interactions** because it evaluates subsets directly.\n",
    "- Optimized for **model performance**, not just statistical correlation.\n",
    "\n",
    "---\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "- **Computationally expensive**, especially with large numbers of features, because multiple models must be trained.\n",
    "- Can **overfit** if the dataset is small, since it optimizes directly on model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Common Wrapper Techniques\n",
    "\n",
    "1. **Forward Selection (Stepwise Forward Selection)**\n",
    "\n",
    "   - Start with no features.\n",
    "   - Add features **one at a time**, keeping the one that improves model performance the most.\n",
    "\n",
    "2. **Backward Elimination (Stepwise Backward Selection)**\n",
    "\n",
    "   - Start with all features.\n",
    "   - Remove features **one at a time**, removing the one whose removal least decreases performance.\n",
    "\n",
    "3. **Exhaustive Search**\n",
    "   - Evaluate **all possible subsets** of features.\n",
    "   - Guarantees the best-performing subset but is **computationally infeasible** for many features.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
